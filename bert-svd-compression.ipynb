{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e0213a9-2d74-4775-8880-11a2b1cf4d23",
   "metadata": {},
   "source": [
    "# Language Model compression with SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b0ed173-a5a1-4073-8a5c-fcc62b20480b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/galinaalperovich/PycharmProjects/modelCompressionFWSVD/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForMaskedLM, DistilBertTokenizer, AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6319951d-6ba5-4670-abaa-377a1b261c29",
   "metadata": {},
   "source": [
    "## Define model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0abe222-50a4-428d-8e11-fd8298d990c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_name = 'distilbert-base-cased'\n",
    "model_name = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8dfc4e-e7ed-4972-b9de-4d10dcd7d14f",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16522300-61f5-4fac-9951-2592df4ba7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff6f27f1-f210-4078-ae76-26883d5ee9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set inference model for the model \n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b81ebb-814d-464c-93e5-40fba976e1c4",
   "metadata": {},
   "source": [
    "## Simple function to visually evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b714cf-1cf3-4d89-ae2f-d1210c862797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    # Prompt text for generation\n",
    "    prompt = \"The quick brown [MASK] jumps over the lazy dog.\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer.encode(prompt, add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "    # Find the position of the [MASK] token\n",
    "    mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1][0]\n",
    "\n",
    "    # Generate predictions for the [MASK] token\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        predictions = outputs[0][0, mask_token_index].topk(5)\n",
    "\n",
    "    # Print the top 5 predicted words\n",
    "    for i, (word_idx, score) in enumerate(zip(predictions.indices, predictions.values)):\n",
    "        word = tokenizer.decode([word_idx])\n",
    "        print(f'{i+1}. {word} ({score:.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea434e7-5f55-4a81-97e2-c69151009a9c",
   "metadata": {},
   "source": [
    "## Truncated SVD for matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eb58027-0125-4284-85e9-6b8e7886886c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def svd(X, k:float=1.0):\n",
    "    \"\"\"\n",
    "    SVD for matrix X, k is the compression level from 0.0 to 1.0, where 1.0 is the original X.\n",
    "    Only make sense computationally if k < U.shape[1]/2\n",
    "    \"\"\"\n",
    "    U, S, V = torch.svd(X)\n",
    "    n = U.shape[1]\n",
    "    k = round(n*k)\n",
    "    if k:\n",
    "        return U[:,:k], S[:k], V[:,:k]\n",
    "    else:\n",
    "        return U, S, V\n",
    "\n",
    "\n",
    "def eval_svd(X, U, S, V):\n",
    "    return (X - U @ S.diag() @ V.T).norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82486462-f82f-4035-b808-69f93f3dfc58",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Simple code to test SVD for a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "071c5ba4-8b99-4fc3-8a2a-692d5a18f2ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = torch.rand(3000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e11a10f-62ba-4230-ab7d-5cf2f4610fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3000, 100])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8e8c2f0-6727-4549-b49e-865d5169395e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=0.1, Norm(X-SVD(X))=147.70260620117188\n",
      "k=0.2, Norm(X-SVD(X))=136.99066162109375\n",
      "k=0.3, Norm(X-SVD(X))=126.0494155883789\n",
      "k=0.5, Norm(X-SVD(X))=103.02754211425781\n",
      "k=0.7, Norm(X-SVD(X))=76.99295806884766\n",
      "k=1, Norm(X-SVD(X))=0.000343029125360772\n"
     ]
    }
   ],
   "source": [
    "for k in [0.1, 0.2, 0.3, 0.5, 0.7, 1]:\n",
    "    U, S, V = svd(X, k=k)\n",
    "    print(\n",
    "        f\"k={k}, Norm(X-SVD(X))={eval_svd(X, U, S, V)}\"\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b5cb9-7f01-4f40-89cf-341b85f4d929",
   "metadata": {},
   "source": [
    "## New Compressed Linear layer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "806a50cb-e50f-4d1d-bee2-3024d35ae476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CompressedLinear(nn.Module):\n",
    "    def __init__(self, U, S, V, b):\n",
    "        super(CompressedLinear, self).__init__()\n",
    "        self.lin1 = nn.Linear(*U.shape, bias=False)\n",
    "        self.lin2 = nn.Linear(*V.T.shape)\n",
    "        \n",
    "        self.lin1.weight = nn.Parameter(V.T)\n",
    "        self.lin2.weight = nn.Parameter(U @ S.diag())\n",
    "        self.lin2.bias = nn.Parameter(b)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.lin2(self.lin1(x))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e500de-3c2a-48cc-8cfc-13ad49445fd5",
   "metadata": {},
   "source": [
    "## Code for model compression \n",
    "Walk agross all layers and replace `Linear` to `CompressedLinear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa693ba0-c2d5-4839-b780-a3d071b5041c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compress_linear(module, k=0.4):\n",
    "    X, b = module.weight, module.bias\n",
    "    U, S, V = svd(X, k=k)\n",
    "    return CompressedLinear(U, S, V, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53227fe9-9db6-48ab-bdc0-5fbc93eea095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compress_model(model: nn.Module, k=0.4):\n",
    "    for name, child in model.named_children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            new_child = compress_linear(child, k=k)\n",
    "            setattr(model, name, new_child)\n",
    "        elif isinstance(child, nn.Module):\n",
    "            compress_model(child, k=k)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a2e4ebf-86f6-4541-b3e8-0e81da337852",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "new_model = compress_model(\n",
    "    AutoModelForMaskedLM.from_pretrained(model_name), k=0.6\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e923b51-56dc-4300-971d-0b35cb01ae79",
   "metadata": {},
   "source": [
    "## Compressed model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2bb9376b-068d-438b-9287-1afcc0033bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. dog (7.49)\n",
      "2. cat (6.67)\n",
      "3. horse (6.29)\n",
      "4. man (6.24)\n",
      "5. bird (5.79)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfcd7ca-d3d1-43a2-90f2-2b9d900d72e3",
   "metadata": {},
   "source": [
    "## Original model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8bf5d221-9178-4ced-8675-15b26965c6d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. dog (12.20)\n",
      "2. ##ie (11.23)\n",
      "3. cat (10.60)\n",
      "4. bear (10.13)\n",
      "5. puppy (10.01)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dcb8c1-427e-43bb-addc-696cfecfd616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e667e12-ae9e-47fe-8101-976a7d311cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
